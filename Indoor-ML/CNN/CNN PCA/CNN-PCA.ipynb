{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.tools import freeze_graph\n",
    "from tensorflow.python.tools import optimize_for_inference_lib\n",
    "from sklearn.decomposition import PCA\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "from six.moves import xrange\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se cargan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()# Training loop\n",
    "Xdf_raw = pd.read_csv(\"../../IndoorFingerprint.csv\").drop(\"Y\", axis=1)\n",
    "Ydf_raw = pd.read_csv(\"../../IndoorFingerprint.csv\").drop(\"X\", axis=1)\n",
    "x = tf.placeholder(tf.float32, shape=[None, 8])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Se escalan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizacion de los datos\n",
    "\n",
    "# Valores para el dataset que predice X\n",
    "Xtrx, Xtex, ytrx, ytex = train_test_split(\n",
    "Xdf_raw.iloc[:, 1:], Xdf_raw[\"X\"], test_size=0.2, random_state=0)\n",
    "train_size = len(Xtrx)\n",
    "test_size = len(Xtex)\n",
    "\n",
    "# Valores para el dataset que predice Y\n",
    "Xtry, Xtey, ytry, ytey = train_test_split(\n",
    "Ydf_raw.iloc[:, 1:], Ydf_raw[\"Y\"], test_size=0.2, random_state=0)\n",
    "\n",
    "# Se normalizan los datos para ambos\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xtrx = scaler.fit_transform(Xtrx)\n",
    "Xtex = scaler.transform(Xtex)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "Xtry = scaler.fit_transform(Xtry)\n",
    "Xtey = scaler.transform(Xtey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Se crea la matriz pca (opcional) x\n",
    "sklearn_pca = PCA(n_components=4)\n",
    "Xtrx = sklearn_pca.fit_transform(Xtrx)\n",
    "Xtex = sklearn_pca.transform(Xtex)\n",
    "\n",
    "sklearn_pca_y = PCA(n_components=4)\n",
    "Xtry = sklearn_pca_y.fit_transform(Xtry)\n",
    "Xtey = sklearn_pca_y.transform(Xtey)\n",
    "\n",
    "atribute_numbers = Xtrx.shape[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71668625  1.67517924 -2.47905397  2.18760347]\n",
      "10\n",
      "[ 2.71668625  1.67517924 -2.47905397  2.18760347]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class_number_x = ytrx.unique().size\n",
    "class_number_y = ytry.unique().size\n",
    "\n",
    "ytrx_dummy = pd.get_dummies(ytrx)\n",
    "ytex_dummy = pd.get_dummies(ytex)\n",
    "\n",
    "ytry_dummy = pd.get_dummies(ytry)\n",
    "ytey_dummy = pd.get_dummies(ytey)\n",
    "\n",
    "# Se convierten valores a Tensores X\n",
    "\n",
    "Xtrx = np.array(Xtrx,dtype='float32')\n",
    "Xtex = np.array(Xtex,dtype='float32')\n",
    "ytrx_dummy = np.array(ytrx_dummy,dtype='int32')\n",
    "ytex_dummy = np.array(ytex_dummy,dtype='int32')\n",
    "\n",
    "# Se convierten valores a Tensores Y\n",
    "\n",
    "Xtry = np.array(Xtry,dtype='float32')\n",
    "Xtey = np.array(Xtey,dtype='float32')\n",
    "ytry_dummy = np.array(ytry_dummy,dtype='int32')\n",
    "ytey_dummy = np.array(ytey_dummy,dtype='int32')\n",
    "\n",
    "ytrx_cat = pd.Series(ytrx, dtype=\"category\")\n",
    "ytrx = np.array(ytrx_cat.cat.codes,dtype='int32')\n",
    "ytex_cat = pd.Series(ytex, dtype=\"category\")\n",
    "ytex = np.array(ytex_cat.cat.codes,dtype='int32')\n",
    "\n",
    "ytry_cat = pd.Series(ytry, dtype=\"category\")\n",
    "ytry = np.array(ytry_cat.cat.codes,dtype='int32')\n",
    "ytey_cat = pd.Series(ytey, dtype=\"category\")\n",
    "ytey = np.array(ytey_cat.cat.codes,dtype='int32')\n",
    "\n",
    "print(Xtrx[20])\n",
    "print(ytrx[20])\n",
    "print(Xtry[20])\n",
    "print(ytry[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "learning_rate = 0.3\n",
    "max_steps = 2000\n",
    "hidden1 = 256\n",
    "hidden2 = 64\n",
    "batch_size = 32\n",
    "input_data_dir = os.path.join(os.getenv('TEST_TMPDIR', '/tmp'),\n",
    "                           'tensorflow/mnist/input_data'),\n",
    "log_dir = os.path.join(os.getenv('TEST_TMPDIR', 'C:/Users/felip/OneDrive/Documentos/Universidad/Memoria/Complementos/Indoor-ML/CNN'),\n",
    "                           'logs/fully_connected_feed')\n",
    "fake_data = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The MNIST dataset has 10 classes, representing the digits 0 through 9.\n",
    "#NUM_CLASSES = class_number_x\n",
    "\n",
    "# The MNIST images are always 28x28 pixels.\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = atribute_numbers\n",
    "\n",
    "\n",
    "def inference(images, hidden1_units, hidden2_units, NUM_CLASSES):\n",
    "    \"\"\"Build the MNIST model up to where it may be used for inference.\n",
    "  Args:\n",
    "    images: Images placeholder, from inputs().\n",
    "    hidden1_units: Size of the first hidden layer.\n",
    "    hidden2_units: Size of the second hidden layer.\n",
    "  Returns:\n",
    "    softmax_linear: Output tensor with the computed logits.\n",
    "  \"\"\"\n",
    "  # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                                stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]),\n",
    "                             name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "  # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden1_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]),\n",
    "                             name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                                stddev=1.0 / math.sqrt(float(hidden2_units))),\n",
    "            name='weights')\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    return logits\n",
    "\n",
    "\n",
    "def lossF(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size].\n",
    "  Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "  \"\"\"\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits, name='xentropy')\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    return cross_entropy_mean\n",
    "\n",
    "\n",
    "def training(loss, learning_rate, mode):\n",
    "    \"\"\"Sets up the training Ops.\n",
    "  Creates a summarizer to track the loss over time in TensorBoard.\n",
    "  Creates an optimizer and applies the gradients to all trainable variables.\n",
    "  The Op returned by this function is what must be passed to the\n",
    "  `sess.run()` call to cause the model to train.\n",
    "  Args:\n",
    "    loss: Loss tensor, from loss().\n",
    "    learning_rate: The learning rate to use for gradient descent.\n",
    "  Returns:\n",
    "    train_op: The Op for training.\n",
    "  \"\"\"\n",
    "    # Add a scalar summary for the snapshot loss.\n",
    "    if mode == 0:\n",
    "        tf.summary.scalar('loss_x', loss)\n",
    "    # Create the gradient descent optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "  Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "  \"\"\"\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    # Return the number of true entries.\n",
    "    accuracy = tf.reduce_sum(tf.cast(correct, tf.int32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "    # Note that the shapes of the placeholders match the shapes of the full\n",
    "    # image and label tensors, except the first dimension is now batch_size\n",
    "    # rather than the full size of the train or test data sets.\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(None,\n",
    "                                                     IMAGE_PIXELS), name = \"I\")\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(None))\n",
    "    return images_placeholder, labels_placeholder\n",
    "\n",
    "\n",
    "def fill_feed_dict(dataX, dataY, images_pl, labels_pl):\n",
    "    \"\"\"Fills the feed_dict for training the given step.\n",
    "    A feed_dict takes the form of:\n",
    "    feed_dict = {\n",
    "      <placeholder>: <tensor of values to be passed for placeholder>,\n",
    "      ....\n",
    "    }\n",
    "    Args:\n",
    "    data_set: The set of images and labels, from input_data.read_data_sets()\n",
    "    images_pl: The images placeholder, from placeholder_inputs().\n",
    "    labels_pl: The labels placeholder, from placeholder_inputs().\n",
    "    Returns:\n",
    "    feed_dict: The feed dictionary mapping from placeholders to values.\n",
    "    \"\"\"\n",
    "    # Create the feed_dict for the placeholders filled with the next\n",
    "    # `batch size` examples.\n",
    "    #images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size,\n",
    "    #                                             FLAGS.fake_data)\n",
    "    \n",
    "    images_feed, labels_feed = next_batch(batch_size, dataX, dataY) \n",
    "                                          \n",
    "    feed_dict = {\n",
    "      images_pl: images_feed,\n",
    "      labels_pl: labels_feed,\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "def do_eval(sess,\n",
    "            eval_correct,\n",
    "            images_placeholder,\n",
    "            labels_placeholder,\n",
    "            dataX, dataY):\n",
    "    \"\"\"Runs one evaluation against the full epoch of data.\n",
    "    Args:\n",
    "    sess: The session in which the model has been trained.\n",
    "    eval_correct: The Tensor that returns the number of correct predictions.\n",
    "    images_placeholder: The images placeholder.\n",
    "    labels_placeholder: The labels placeholder.\n",
    "    data_set: The set of images and labels to evaluate, from\n",
    "      input_data.read_data_sets().\n",
    "    \"\"\"\n",
    "    # And run one epoch of eval.\n",
    "    true_count = 0  # Counts the number of correct predictions.\n",
    "    steps_per_epoch = dataX.shape[0] // batch_size\n",
    "    num_examples = steps_per_epoch * batch_size\n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(dataX, dataY, images_placeholder,\n",
    "                                   labels_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict=feed_dict)\n",
    "    precision = float(true_count) / num_examples\n",
    "    print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "        (num_examples, true_count, precision))\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    \"\"\"Train MNIST for a number of steps.\"\"\"\n",
    "    # Get the sets of images and labels for training, validation, and\n",
    "    # test on MNIST.\n",
    "    #data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "\n",
    "    # Tell TensorFlow that the model will be built into the default Graph.\n",
    "    with tf.Graph().as_default():\n",
    "        # Generate placeholders for the images and labels.\n",
    "        images_placeholder, labels_placeholder_x = placeholder_inputs(batch_size)\n",
    "        _, labels_placeholder_y = placeholder_inputs(batch_size)\n",
    "\n",
    "        # Build a Graph that computes predictions from the inference model.\n",
    "        logits_x = inference(images_placeholder,hidden1,hidden2,class_number_x)\n",
    "        logits_y = inference(images_placeholder,hidden1,hidden2,class_number_y)\n",
    "\n",
    "        # Add to the Graph the Ops for loss calculation.\n",
    "        loss_x = lossF(logits_x, labels_placeholder_x)\n",
    "        loss_y = lossF(logits_y, labels_placeholder_y)\n",
    "\n",
    "        # Add to the Graph the Ops that calculate and apply gradients.\n",
    "        train_op_x = training(loss_x, learning_rate,0)\n",
    "        train_op_y = training(loss_y, learning_rate,1)\n",
    "\n",
    "        # Add the Op to compare the logits to the labels during evaluation.\n",
    "        eval_correct_x = evaluation(logits_x, labels_placeholder_x)\n",
    "        eval_correct_y = evaluation(logits_y, labels_placeholder_y)\n",
    "        \n",
    "        _, accuracy_x = tf.metrics.accuracy(labels_placeholder_x, tf.argmax(logits_x, 1))\n",
    "        accuracy_summary_x = tf.summary.scalar('accuracy_x', accuracy_x)\n",
    "        \n",
    "        _, accuracy_y = tf.metrics.accuracy(labels_placeholder_y, tf.argmax(logits_y, 1))\n",
    "        #accuracy_summary_y = tf.summary.scalar('accuracy_y', accuracy_y)\n",
    "\n",
    "        # Build the summary Tensor based on the TF collection of Summaries.\n",
    "        summary = tf.summary.merge_all()\n",
    "\n",
    "        # Add the variable initializer Op.\n",
    "        init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "        # Create a saver for writing training checkpoints.\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Create a session for running Ops on the Graph.\n",
    "        sess = tf.Session()\n",
    "\n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter(log_dir+ \"/train\")\n",
    "        test_writer = tf.summary.FileWriter(log_dir + '/test')\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "\n",
    "        # And then after everything is built:\n",
    "\n",
    "        # Run the Op to initialize the variables.\n",
    "        sess.run(init)\n",
    "\n",
    "        # Start the training loop.\n",
    "        for step in xrange(max_steps):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Fill a feed dictionary with the actual set of images and labels\n",
    "            # for this particular training step.\n",
    "            feed_dict_x = fill_feed_dict(Xtrx, ytrx, images_placeholder,\n",
    "                                     labels_placeholder_x)\n",
    "            \n",
    "            feed_dict_y = fill_feed_dict(Xtry, ytry, images_placeholder,\n",
    "                                     labels_placeholder_y)\n",
    "\n",
    "            # Run one step of the model.  The return values are the activations\n",
    "            # from the `train_op` (which is discarded) and the `loss` Op.  To\n",
    "            # inspect the values of your Ops or variables, you may include them\n",
    "            # in the list passed to sess.run() and the value tensors will be\n",
    "            # returned in the tuple from the call.\n",
    "            _, loss_value_x, accuracy_value_x = sess.run([train_op_x, loss_x, accuracy_x],\n",
    "                                   feed_dict=feed_dict_x)\n",
    "            \n",
    "            _, loss_value_y, accuracy_value_y = sess.run([train_op_y, loss_y, accuracy_y],\n",
    "                                   feed_dict=feed_dict_y)\n",
    "        \n",
    "            duration = time.time() - start_time\n",
    "\n",
    "            # Write the summaries and print an overview fairly often.\n",
    "            if step % 100 == 0:\n",
    "                # Print status to stdout.\n",
    "                # Update the events file.\n",
    "                summary_str_x = sess.run(summary, feed_dict=feed_dict_x)\n",
    "                summary_writer.add_summary(summary_str_x, step)\n",
    "                summary_test_x, accuracy_value_test_x = sess.run([accuracy_summary_x, accuracy_x], feed_dict = {images_placeholder: Xtex, labels_placeholder_x: ytex})\n",
    "                accuracy_value_test_y = sess.run(accuracy_y, feed_dict = {images_placeholder: Xtey, labels_placeholder_y: ytey})\n",
    "                test_writer.add_summary(summary_test_x, step)\n",
    "                summary_writer.flush()\n",
    "                print('Step %d: accuracy = %.2f (%.3f sec)' % (step, accuracy_value_x, duration))\n",
    "                print('Step %d: accuracy test = %.2f (%.3f sec)' % (step, accuracy_value_test_x, duration))\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value_x, duration))\n",
    "                print(\"--------------------------------------------------------------------------------------\")\n",
    "                print('Step %d: accuracy = %.2f (%.3f sec)' % (step, accuracy_value_y, duration))\n",
    "                print('Step %d: accuracy test = %.2f (%.3f sec)' % (step, accuracy_value_test_y, duration))\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value_y, duration))\n",
    "                print(\"############################################################################################\")\n",
    "\n",
    "            # Save a checkpoint and evaluate the model periodically.\n",
    "            if (step + 1) % 1000 == 0 or (step + 1) == max_steps:\n",
    "                checkpoint_file = os.path.join(log_dir, 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_file, global_step=step)\n",
    "                # Evaluate against the training set.\n",
    "                print('Training Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct_x,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder_x,\n",
    "                        Xtrx, ytrx)\n",
    "                # Evaluate against the validation set.\n",
    "                # Evaluate against the test set.\n",
    "                print('Test Data Eval:')\n",
    "                do_eval(sess,\n",
    "                        eval_correct_x,\n",
    "                        images_placeholder,\n",
    "                        labels_placeholder_x,\n",
    "                        Xtex, ytex)\n",
    "        \n",
    "        output_node_x = tf.cast(tf.argmax(logits_x, 1), tf.int32)\n",
    "        output_node_y = tf.cast(tf.argmax(logits_y, 1), tf.int32)\n",
    "        output_xy = tf.concat([output_node_x, output_node_y], 0, name=\"O\")\n",
    "        tf.train.write_graph(sess.graph_def, '.', 'NN_PCA.pbtxt')\n",
    "        saver.save(sess, './NN_PCA.ckpt')\n",
    "        #correct_prediction = tf.argmax(sess.run(logits, feed_dict = {images_placeholder: [[2.80714798, -0.36167914, -1.0649991,   4.10078526, -0.44053763,  0.52955198\n",
    "#,  0.41685054,  1.31442308]]}))\n",
    "        print(sess.run(output_xy, feed_dict = {images_placeholder: [[2.71668625,  1.67517924, -2.47905397,  2.18760347]]}))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: accuracy = 0.12 (0.069 sec)\n",
      "Step 0: accuracy test = 0.28 (0.069 sec)\n",
      "Step 0: loss = 2.41 (0.069 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 0: accuracy = 0.12 (0.069 sec)\n",
      "Step 0: accuracy test = 0.34 (0.069 sec)\n",
      "Step 0: loss = 1.57 (0.069 sec)\n",
      "############################################################################################\n",
      "Step 100: accuracy = 0.41 (0.005 sec)\n",
      "Step 100: accuracy test = 0.44 (0.005 sec)\n",
      "Step 100: loss = 1.11 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 100: accuracy = 0.56 (0.005 sec)\n",
      "Step 100: accuracy test = 0.60 (0.005 sec)\n",
      "Step 100: loss = 0.59 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 200: accuracy = 0.49 (0.004 sec)\n",
      "Step 200: accuracy test = 0.51 (0.004 sec)\n",
      "Step 200: loss = 0.72 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 200: accuracy = 0.64 (0.004 sec)\n",
      "Step 200: accuracy test = 0.66 (0.004 sec)\n",
      "Step 200: loss = 0.54 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 300: accuracy = 0.54 (0.005 sec)\n",
      "Step 300: accuracy test = 0.55 (0.005 sec)\n",
      "Step 300: loss = 0.76 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 300: accuracy = 0.69 (0.005 sec)\n",
      "Step 300: accuracy test = 0.70 (0.005 sec)\n",
      "Step 300: loss = 0.73 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 400: accuracy = 0.57 (0.004 sec)\n",
      "Step 400: accuracy test = 0.58 (0.004 sec)\n",
      "Step 400: loss = 0.49 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 400: accuracy = 0.71 (0.004 sec)\n",
      "Step 400: accuracy test = 0.72 (0.004 sec)\n",
      "Step 400: loss = 0.38 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 500: accuracy = 0.59 (0.005 sec)\n",
      "Step 500: accuracy test = 0.60 (0.005 sec)\n",
      "Step 500: loss = 1.03 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 500: accuracy = 0.73 (0.005 sec)\n",
      "Step 500: accuracy test = 0.73 (0.005 sec)\n",
      "Step 500: loss = 0.44 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 600: accuracy = 0.61 (0.005 sec)\n",
      "Step 600: accuracy test = 0.62 (0.005 sec)\n",
      "Step 600: loss = 0.69 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 600: accuracy = 0.74 (0.005 sec)\n",
      "Step 600: accuracy test = 0.74 (0.005 sec)\n",
      "Step 600: loss = 0.85 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 700: accuracy = 0.63 (0.004 sec)\n",
      "Step 700: accuracy test = 0.63 (0.004 sec)\n",
      "Step 700: loss = 0.60 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 700: accuracy = 0.75 (0.004 sec)\n",
      "Step 700: accuracy test = 0.76 (0.004 sec)\n",
      "Step 700: loss = 0.35 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 800: accuracy = 0.64 (0.005 sec)\n",
      "Step 800: accuracy test = 0.64 (0.005 sec)\n",
      "Step 800: loss = 0.85 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 800: accuracy = 0.76 (0.005 sec)\n",
      "Step 800: accuracy test = 0.76 (0.005 sec)\n",
      "Step 800: loss = 0.39 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 900: accuracy = 0.65 (0.004 sec)\n",
      "Step 900: accuracy test = 0.66 (0.004 sec)\n",
      "Step 900: loss = 0.68 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 900: accuracy = 0.77 (0.004 sec)\n",
      "Step 900: accuracy test = 0.77 (0.004 sec)\n",
      "Step 900: loss = 0.34 (0.004 sec)\n",
      "############################################################################################\n",
      "Training Data Eval:\n",
      "  Num examples: 5280  Num correct: 4135  Precision @ 1: 0.7831\n",
      "Test Data Eval:\n",
      "  Num examples: 1312  Num correct: 1016  Precision @ 1: 0.7744\n",
      "Step 1000: accuracy = 0.66 (0.005 sec)\n",
      "Step 1000: accuracy test = 0.67 (0.005 sec)\n",
      "Step 1000: loss = 0.56 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1000: accuracy = 0.78 (0.005 sec)\n",
      "Step 1000: accuracy test = 0.78 (0.005 sec)\n",
      "Step 1000: loss = 0.14 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1100: accuracy = 0.67 (0.005 sec)\n",
      "Step 1100: accuracy test = 0.68 (0.005 sec)\n",
      "Step 1100: loss = 0.59 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1100: accuracy = 0.78 (0.005 sec)\n",
      "Step 1100: accuracy test = 0.78 (0.005 sec)\n",
      "Step 1100: loss = 0.25 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1200: accuracy = 0.68 (0.004 sec)\n",
      "Step 1200: accuracy test = 0.68 (0.004 sec)\n",
      "Step 1200: loss = 0.61 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1200: accuracy = 0.79 (0.004 sec)\n",
      "Step 1200: accuracy test = 0.79 (0.004 sec)\n",
      "Step 1200: loss = 0.37 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 1300: accuracy = 0.69 (0.005 sec)\n",
      "Step 1300: accuracy test = 0.69 (0.005 sec)\n",
      "Step 1300: loss = 0.41 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1300: accuracy = 0.79 (0.005 sec)\n",
      "Step 1300: accuracy test = 0.79 (0.005 sec)\n",
      "Step 1300: loss = 0.33 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1400: accuracy = 0.70 (0.005 sec)\n",
      "Step 1400: accuracy test = 0.70 (0.005 sec)\n",
      "Step 1400: loss = 0.62 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1400: accuracy = 0.80 (0.005 sec)\n",
      "Step 1400: accuracy test = 0.80 (0.005 sec)\n",
      "Step 1400: loss = 0.41 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1500: accuracy = 0.70 (0.005 sec)\n",
      "Step 1500: accuracy test = 0.71 (0.005 sec)\n",
      "Step 1500: loss = 0.34 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1500: accuracy = 0.80 (0.005 sec)\n",
      "Step 1500: accuracy test = 0.80 (0.005 sec)\n",
      "Step 1500: loss = 0.37 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1600: accuracy = 0.71 (0.005 sec)\n",
      "Step 1600: accuracy test = 0.71 (0.005 sec)\n",
      "Step 1600: loss = 0.44 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1600: accuracy = 0.80 (0.005 sec)\n",
      "Step 1600: accuracy test = 0.81 (0.005 sec)\n",
      "Step 1600: loss = 0.33 (0.005 sec)\n",
      "############################################################################################\n",
      "Step 1700: accuracy = 0.71 (0.004 sec)\n",
      "Step 1700: accuracy test = 0.72 (0.004 sec)\n",
      "Step 1700: loss = 0.46 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1700: accuracy = 0.81 (0.004 sec)\n",
      "Step 1700: accuracy test = 0.81 (0.004 sec)\n",
      "Step 1700: loss = 0.36 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 1800: accuracy = 0.72 (0.004 sec)\n",
      "Step 1800: accuracy test = 0.72 (0.004 sec)\n",
      "Step 1800: loss = 0.76 (0.004 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1800: accuracy = 0.81 (0.004 sec)\n",
      "Step 1800: accuracy test = 0.81 (0.004 sec)\n",
      "Step 1800: loss = 0.33 (0.004 sec)\n",
      "############################################################################################\n",
      "Step 1900: accuracy = 0.72 (0.005 sec)\n",
      "Step 1900: accuracy test = 0.72 (0.005 sec)\n",
      "Step 1900: loss = 0.83 (0.005 sec)\n",
      "--------------------------------------------------------------------------------------\n",
      "Step 1900: accuracy = 0.81 (0.005 sec)\n",
      "Step 1900: accuracy test = 0.81 (0.005 sec)\n",
      "Step 1900: loss = 0.47 (0.005 sec)\n",
      "############################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Eval:\n",
      "  Num examples: 5280  Num correct: 4402  Precision @ 1: 0.8337\n",
      "Test Data Eval:\n",
      "  Num examples: 1312  Num correct: 1034  Precision @ 1: 0.7881\n",
      "[10  1]\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./NN_PCA.ckpt\n",
      "INFO:tensorflow:Froze 12 variables.\n",
      "Converted 12 variables to const ops.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'NN_PCA'\n",
    "\n",
    "# Freeze the graph\n",
    "\n",
    "input_graph_path = MODEL_NAME+'.pbtxt'\n",
    "checkpoint_path = './'+MODEL_NAME+'.ckpt'\n",
    "input_saver_def_path = \"\"\n",
    "input_binary = False\n",
    "output_node_names = \"O\"\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "output_frozen_graph_name = 'frozen_'+MODEL_NAME+'.pb'\n",
    "output_optimized_graph_name = 'optimized_'+MODEL_NAME+'.pb'\n",
    "clear_devices = True\n",
    "\n",
    "\n",
    "freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n",
    "                          input_binary, checkpoint_path, output_node_names,\n",
    "                          restore_op_name, filename_tensor_name,\n",
    "                          output_frozen_graph_name, clear_devices, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizacion\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.Open(output_frozen_graph_name, \"rb\") as f:\n",
    "    data = f.read()\n",
    "    input_graph_def.ParseFromString(data)\n",
    "\n",
    "output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n",
    "        input_graph_def,\n",
    "        [\"I\"], # an array of the input node(s)\n",
    "        [\"O\"], # an array of output nodes\n",
    "        tf.float32.as_datatype_enum)\n",
    "\n",
    "# Save the optimized graph\n",
    "\n",
    "f = tf.gfile.FastGFile(output_optimized_graph_name, \"w\")\n",
    "f.write(output_graph_def.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
